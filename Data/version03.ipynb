{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/praveen/MSDS/MachineLearning/KaggleChallenges/Challenge2/Data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/yfvk0l4921v0kg94n8qt42wc0000gn/T/ipykernel_65768/2922911961.py:1: DtypeWarning: Columns (31,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"./train_data_new.csv\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./train_data_new.csv\")\n",
    "test = pd.read_csv(\"./test_data_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/yfvk0l4921v0kg94n8qt42wc0000gn/T/ipykernel_65768/3694093064.py:1: DtypeWarning: Columns (31,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"./train_data_new.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute difference between original and recomputed ratios for train and test data:\n",
      "  candidacy_democratic_ratio               0.00000\n",
      "  candidacy_democratic_ratio               0.00000\n",
      "  candidacy_republican_ratio               0.00000\n",
      "  candidacy_republican_ratio               0.00000\n",
      "  contribution_democratic_count_ratio      0.00000\n",
      "  contribution_democratic_count_ratio      0.00000\n",
      "  contribution_republican_count_ratio      0.00000\n",
      "  contribution_republican_count_ratio      0.00000\n",
      "  politician_challenger_ratio              0.00000\n",
      "  politician_challenger_ratio              0.00000\n",
      "  politician_democratic_ratio              0.00000\n",
      "  politician_democratic_ratio              0.00000\n",
      "  politician_incumbency_ratio              0.00000\n",
      "  politician_incumbency_ratio              0.00000\n",
      "  politician_open_pos_ratio                0.00000\n",
      "  politician_open_pos_ratio                0.00000\n",
      "  politician_republican_ratio              0.00000\n",
      "  politician_republican_ratio              0.00000\n",
      "  governor_contribution_ratio              0.00023\n",
      "  governor_contribution_ratio              0.00036\n",
      "  house_and_assembly_contribution_ratio    inf\n",
      "  house_and_assembly_contribution_ratio    0.00094\n",
      "  senate_contribution_ratio                0.00057\n",
      "  senate_contribution_ratio                0.00108\n",
      "  us_house_contribution_ratio              inf\n",
      "  us_house_contribution_ratio              0.00097\n",
      "  us_senate_contribution_ratio             0.00053\n",
      "  us_senate_contribution_ratio             0.00191\n",
      "‚úÖ All ratios safely overwritten in place and checked.\n",
      "üîç Number of duplicate rows in train: 2417\n",
      "üîç Number of duplicate rows in test: 2417\n",
      "‚úÖ Duplicates removed for train. New shape: (170431, 57)\n",
      "‚úÖ Duplicates removed for test. New shape: (115232, 58)\n",
      "‚úÖ  Skew & kurtosis treatments applied for train and test.\n",
      "‚úÖ  Pre‚Äëprocessing finished. Shapes: (170431, 58)\n",
      "‚úÖ  Pre‚Äëprocessing finished. Shapes: (115232, 57)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./train_data_new.csv\")\n",
    "test = pd.read_csv(\"./test_data_new.csv\")\n",
    "cat_vars = ['general_sector', 'city', 'zip_code', 'specific_sector', 'state',\n",
    "       'contributor_type']\n",
    "train_num_vars = ['winner_ratio', 'candidacy_count',\n",
    "       'candidacy_democratic_count', 'candidacy_republican_count',\n",
    "       'contribution_count', 'contribution_democratic_count',\n",
    "       'contribution_republican_count', 'politician_challenger_count',\n",
    "       'politician_count', 'politician_democratic_count',\n",
    "       'politician_incumbency_count', 'politician_open_pos_count',\n",
    "       'politician_republican_count', 'contribution_democratic_sum_2010_usd',\n",
    "       'contribution_republican_sum_2010_usd', 'contribution_sum_2010_usd',\n",
    "       'governor_contributions_sum_2010_usd',\n",
    "       'house_and_assembly_contributions_sum_2010_usd',\n",
    "       'senate_contributions_sum_2010_usd',\n",
    "       'us_house_contributions_sum_2010_usd',\n",
    "       'us_senate_contributions_sum_2010_usd', 'candidacy_democratic_ratio',\n",
    "       'candidacy_republican_ratio', 'contribution_democratic_count_ratio',\n",
    "       'contribution_republican_count_ratio', 'governor_contribution_ratio',\n",
    "       'house_and_assembly_contribution_ratio', 'politician_challenger_ratio',\n",
    "       'politician_democratic_ratio', 'politician_incumbency_ratio',\n",
    "       'politician_open_pos_ratio', 'politician_republican_ratio',\n",
    "       'senate_contribution_ratio', 'us_house_contribution_ratio',\n",
    "       'us_senate_contribution_ratio', 'contrib_state_deg',\n",
    "       'contrib_state_wdeg', 'contrib_state_pr', 'contrib_state_comm',\n",
    "       'contrib_fed_deg', 'contrib_fed_wdeg', 'contrib_fed_pr',\n",
    "       'contrib_fed_comm', 'state_all_deg', 'state_all_wdeg', 'state_all_pr',\n",
    "       'state_all_comm', 'state_win_deg', 'state_win_wdeg', 'state_win_pr',\n",
    "       'state_win_comm']\n",
    "test_num_vars = [col for col in train_num_vars if col != \"winner_ratio\"]\n",
    "\n",
    "train[train_num_vars] = train[train_num_vars].apply(pd.to_numeric, errors='coerce')\n",
    "test[test_num_vars] = test[test_num_vars].apply(pd.to_numeric, errors='coerce')\n",
    "cols_with_negatives = ['contribution_democratic_sum_2010_usd','contribution_republican_sum_2010_usd','contribution_sum_2010_usd','governor_contributions_sum_2010_usd',\n",
    "'house_and_assembly_contributions_sum_2010_usd','senate_contributions_sum_2010_usd','us_house_contributions_sum_2010_usd',\n",
    "'us_senate_contributions_sum_2010_usd','governor_contribution_ratio','house_and_assembly_contribution_ratio','senate_contribution_ratio',\n",
    "'us_house_contribution_ratio','us_senate_contribution_ratio']\n",
    "\n",
    "### Treat these descrepent columns\n",
    "for col in cols_with_negatives:\n",
    "    train[col] = train[col].clip(lower=0)\n",
    "    test[col] = test[col].clip(lower=0)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) Define your ratio definitions (old_name, numerator, denominator)\n",
    "ratio_defs = [\n",
    "    # count‚Äëbased\n",
    "    (\"candidacy_democratic_ratio\",   \"candidacy_democratic_count\",            \"candidacy_count\"),\n",
    "    (\"candidacy_republican_ratio\",   \"candidacy_republican_count\",            \"candidacy_count\"),\n",
    "    (\"contribution_democratic_count_ratio\",\"contribution_democratic_count\",\"contribution_count\"),\n",
    "    (\"contribution_republican_count_ratio\",\"contribution_republican_count\",\"contribution_count\"),\n",
    "    (\"politician_challenger_ratio\",  \"politician_challenger_count\",           \"politician_count\"),\n",
    "    (\"politician_democratic_ratio\",  \"politician_democratic_count\",           \"politician_count\"),\n",
    "    (\"politician_incumbency_ratio\",  \"politician_incumbency_count\",           \"politician_count\"),\n",
    "    (\"politician_open_pos_ratio\",    \"politician_open_pos_count\",             \"politician_count\"),\n",
    "    (\"politician_republican_ratio\",  \"politician_republican_count\",           \"politician_count\"),\n",
    "    # dollar‚Äësum based\n",
    "    (\"governor_contribution_ratio\",  \"governor_contributions_sum_2010_usd\",   \"contribution_sum_2010_usd\"),\n",
    "    (\"house_and_assembly_contribution_ratio\",\"house_and_assembly_contributions_sum_2010_usd\",\"contribution_sum_2010_usd\"),\n",
    "    (\"senate_contribution_ratio\",    \"senate_contributions_sum_2010_usd\",      \"contribution_sum_2010_usd\"),\n",
    "    (\"us_house_contribution_ratio\",  \"us_house_contributions_sum_2010_usd\",    \"contribution_sum_2010_usd\"),\n",
    "    (\"us_senate_contribution_ratio\", \"us_senate_contributions_sum_2010_usd\",   \"contribution_sum_2010_usd\"),\n",
    "]\n",
    "\n",
    "# 2) (Optional) Save the originals to compare later\n",
    "orig_train = train[[old for old,_,_ in ratio_defs]].copy()\n",
    "orig_test = test[[old for old,_,_ in ratio_defs]].copy()\n",
    "\n",
    "# 3) Overwrite each ratio directly, safely\n",
    "for oldcol, num, den in ratio_defs:\n",
    "    train[oldcol] = (\n",
    "        train[num]\n",
    "            .div(train[den].replace({0: np.nan}))  # 0‚ÜíNaN\n",
    "            .fillna(0)                             # NaN‚Üí0\n",
    "            .clip(0, 1)                            # clamp into [0,1]\n",
    "    )\n",
    "    test[oldcol] = (\n",
    "        test[num]\n",
    "            .div(test[den].replace({0: np.nan}))  # 0‚ÜíNaN\n",
    "            .fillna(0)                             # NaN‚Üí0\n",
    "            .clip(0, 1)                            # clamp into [0,1]\n",
    "    )\n",
    "\n",
    "\n",
    "# 4) Print average absolute differences vs. originals\n",
    "print(\"Average absolute difference between original and recomputed ratios for train and test data:\")\n",
    "for oldcol, _, _ in ratio_defs:\n",
    "    if oldcol in orig_train:\n",
    "        delta_train = (train[oldcol] - orig_train[oldcol]).abs().mean()\n",
    "        delta_test = (test[oldcol] - orig_test[oldcol]).abs().mean()\n",
    "        print(f\"  {oldcol:40s} {delta_train:.5f}\")\n",
    "        print(f\"  {oldcol:40s} {delta_test:.5f}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"  {oldcol:40s} (original not present)\")\n",
    "\n",
    "# 5) Sanity‚Äëcheck\n",
    "assert np.isfinite(train[[old for old,_,_ in ratio_defs]]).all().all(), \"Still found infinities!\"\n",
    "assert np.isfinite(test[[old for old,_,_ in ratio_defs]]).all().all(), \"Still found infinities!\"\n",
    "\n",
    "assert ((train[[old for old,_,_ in ratio_defs]] >= 0) & (train[[old for old,_,_ in ratio_defs]] <= 1)).all().all(), \"Ratios out of [0,1]!\"\n",
    "assert ((test[[old for old,_,_ in ratio_defs]] >= 0) & (test[[old for old,_,_ in ratio_defs]] <= 1)).all().all(), \"Ratios out of [0,1]!\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ All ratios safely overwritten in place and checked.\")\n",
    "\n",
    "\n",
    "# 1. Check how many duplicate rows exist\n",
    "num_duplicates_train = train.duplicated().sum()\n",
    "num_duplicates_test = train.duplicated().sum()\n",
    "\n",
    "print(f\"üîç Number of duplicate rows in train: {num_duplicates_train}\")\n",
    "print(f\"üîç Number of duplicate rows in test: {num_duplicates_test}\")\n",
    "\n",
    "# 2. Drop duplicate records\n",
    "train = train.drop_duplicates()\n",
    "test = test.drop_duplicates()\n",
    "\n",
    "print(f\"‚úÖ Duplicates removed for train. New shape: {train.shape}\")\n",
    "print(f\"‚úÖ Duplicates removed for test. New shape: {test.shape}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0.  Lists from above skew table\n",
    "# ------------------------------------------------------------\n",
    "money_like = [\n",
    "    'contribution_republican_sum_2010_usd','governor_contributions_sum_2010_usd',\n",
    "    'contribution_republican_sum_2010_usd','contribution_sum_2010_usd',\n",
    "    'senate_contributions_sum_2010_usd','house_and_assembly_contributions_sum_2010_usd',\n",
    "    'us_senate_contributions_sum_2010_usd','contribution_democratic_sum_2010_usd',\n",
    "    'us_house_contributions_sum_2010_usd'\n",
    "]\n",
    "\n",
    "count_like = [\n",
    "    'contribution_republican_count','contribution_count',\n",
    "    'contribution_democratic_count','candidacy_democratic_count',\n",
    "    'candidacy_republican_count','candidacy_count',\n",
    "    'politician_open_pos_count','politician_republican_count',\n",
    "    'politician_incumbency_count','politician_democratic_count',\n",
    "    'politician_count','politician_challenger_count'\n",
    "]\n",
    "\n",
    "ratio_like = [\n",
    "    'us_senate_contribution_ratio','us_house_contribution_ratio',\n",
    "    'politician_challenger_ratio','senate_contribution_ratio','state_all_comm'\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  log1p transform for money & count cols\n",
    "# ------------------------------------------------------------\n",
    "for col in money_like + count_like:\n",
    "    if col in train.columns:           # guard against typos / dropped cols\n",
    "        train[col] = np.log1p(train[col].clip(lower=0))\n",
    "        test[col]  = np.log1p(test[col].clip(lower=0))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Yeo‚ÄëJohnson power transform for skewed ratios\n",
    "# ------------------------------------------------------------\n",
    "pt = PowerTransformer(method=\"yeo-johnson\", standardize=False)\n",
    "train[ratio_like] = pt.fit_transform(train[ratio_like])\n",
    "test[ratio_like]  = pt.transform(test[ratio_like])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Winsorise: clip any extreme 1¬†% tails on ALL numeric cols\n",
    "# ------------------------------------------------------------\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if col == \"winner_ratio\":\n",
    "        lo, hi = np.percentile(train[col], [1, 99])\n",
    "        train[col] = train[col].clip(lo, hi)\n",
    "    else:\n",
    "        lo, hi = np.percentile(train[col], [1, 99])\n",
    "        train[col] = train[col].clip(lo, hi)\n",
    "        test[col]  = test[col].clip(lo, hi)\n",
    "\n",
    "print(\"‚úÖ  Skew & kurtosis treatments applied for train and test.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  PARAMETERS\n",
    "# ------------------------------------------------------------------\n",
    "TOP_GENERAL   = 25\n",
    "TOP_SPECIFIC  = 100\n",
    "STATE_MIN     = 200          # states with <200 rows ‚Üí OTHER\n",
    "TE_SMOOTHING  = 10           # strength of shrinkage in TargetEncoder\n",
    "\n",
    "cat_top_lump  = {\n",
    "    \"general_sector\"  : (TOP_GENERAL,  \"Other_Sector\"),\n",
    "    \"specific_sector\" : (TOP_SPECIFIC, \"Other_Specific\"),\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  HELPER FUNCTIONS\n",
    "# ------------------------------------------------------------------\n",
    "def lump_by_top(df, col, top_n, new_label):\n",
    "    top_vals = df[col].value_counts().nlargest(top_n).index\n",
    "    df[col]  = df[col].where(df[col].isin(top_vals), new_label)\n",
    "    return df\n",
    "\n",
    "def lump_by_min(df, col, min_cnt, new_label):\n",
    "    small = df[col].value_counts()[lambda s: s < min_cnt].index\n",
    "    df[col] = df[col].where(~df[col].isin(small), new_label)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  APPLY LUMPING¬†(***train and test***)\n",
    "# ------------------------------------------------------------------\n",
    "for col, (top_n, label) in cat_top_lump.items():\n",
    "    train = lump_by_top(train, col, top_n, label)\n",
    "    test  = lump_by_top(test,  col, top_n, label)   # ‚òÖ keep mapping identical\n",
    "\n",
    "train = lump_by_min(train, \"state\", STATE_MIN, \"OTHER\")\n",
    "test  = lump_by_min(test,  \"state\", STATE_MIN, \"OTHER\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  TARGET ENCODING  (***fit on train, transform test***)\n",
    "# ------------------------------------------------------------------\n",
    "te_cols = [\"general_sector\", \"specific_sector\", \"state\"]\n",
    "te      = TargetEncoder(cols=te_cols, smoothing=TE_SMOOTHING)\n",
    "\n",
    "train[te_cols] = te.fit_transform(train[te_cols], train[\"winner_ratio\"])\n",
    "test[te_cols]  = te.transform(test[te_cols])            # ‚òÖ no fit here\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5  FREQUENCY‚ÄëENCODE city and zip_code  (train‚Äëfit, test‚Äëtransform)\n",
    "# ------------------------------------------------------------------\n",
    "for col in [\"city\", \"zip_code\"]:\n",
    "    freq = (train[col].astype(str)      # ensure consistent dtype\n",
    "                    .value_counts(normalize=True))   # share of total rows\n",
    "\n",
    "    train[f\"{col}_freq\"] = train[col].astype(str).map(freq).fillna(0)\n",
    "    test[f\"{col}_freq\"]  = test[col].astype(str).map(freq).fillna(0)\n",
    "\n",
    "# drop the raw string columns (high cardinality)\n",
    "train.drop(columns=[\"city\", \"zip_code\"], inplace=True)\n",
    "test.drop(columns=[\"city\",  \"zip_code\"], inplace=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.  HANDLE THE REMAINING SMALL CATEGORICALS WITH ONE‚ÄëHOT\n",
    "# ------------------------------------------------------------------\n",
    "cat_small = [\"contributor_type\"]      # only 3 levels\n",
    "train = pd.get_dummies(train, columns=cat_small, drop_first=True)\n",
    "test  = pd.get_dummies(test,  columns=cat_small, drop_first=True)\n",
    "\n",
    "# Keep column order identical in train & test after one‚Äëhot:\n",
    "missing_cols = [c for c in train.columns if c not in test.columns]\n",
    "for c in missing_cols:\n",
    "    print(f'{c} column is missing in test data')\n",
    "    test[c] = 0\n",
    "test = test[train.columns.drop(\"winner_ratio\")]         # align\n",
    "\n",
    "print(\"‚úÖ  Pre‚Äëprocessing finished. Shapes:\", train.shape)\n",
    "print(\"‚úÖ  Pre‚Äëprocessing finished. Shapes:\",   test.shape)\n",
    "   \n",
    "for col in [\"city\", \"zip_code\",\"contributor_type\"]:\n",
    "    if col in cat_vars:\n",
    "        cat_vars.remove(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ‚îÄ‚îÄ 1) FEATURES & TARGET ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "FEATURES = [\n",
    "    'politician_incumbency_ratio',\n",
    "    'politician_incumbency_count',\n",
    "    'politician_challenger_ratio',\n",
    "    'politician_open_pos_ratio',\n",
    "    'politician_challenger_count',\n",
    "    'politician_open_pos_count',\n",
    "    'governor_contribution_ratio',\n",
    "    'contribution_sum_2010_usd',\n",
    "    'candidacy_count',\n",
    "    'house_and_assembly_contributions_sum_2010_usd',\n",
    "    'senate_contributions_sum_2010_usd',\n",
    "    'senate_contribution_ratio',\n",
    "    'governor_contributions_sum_2010_usd',\n",
    "    'house_and_assembly_contribution_ratio',\n",
    "    'politician_count',\n",
    "    'candidacy_republican_count',\n",
    "    'politician_republican_count',\n",
    "    'contribution_republican_count',\n",
    "    'contribution_count',\n",
    "    'contribution_republican_sum_2010_usd'\n",
    "]\n",
    "\n",
    "X = train[FEATURES]\n",
    "y = train[\"winner_ratio\"]\n",
    "X_test = test[FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1\n",
      "Metal device set to: Apple M4\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 11:28:11.645317: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-17 11:28:11.645438: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-04-17 11:28:11.966971: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2025-04-17 11:28:12.139650: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-04-17 11:28:14.022608: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 11:31:41.324716: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/533 [==============================] - 1s 1ms/step\n",
      "134/134 [==============================] - 0s 979us/step\n",
      "   fold 1 TRAIN MSE = 0.076006\n",
      "   fold 1 VAL   MSE = 0.085509\n",
      "   fold¬†1 MSE = 0.085509\n",
      "451/451 [==============================] - 0s 1ms/step\n",
      "\n",
      "üìÇ Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 11:31:43.427362: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-04-17 11:31:45.267311: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m net \u001b[38;5;241m=\u001b[39m make_model(X_tr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     59\u001b[0m es  \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     60\u001b[0m         patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 3‚Äëc  out‚Äëof‚Äëfold preds & score\u001b[39;00m\n\u001b[1;32m     67\u001b[0m oof[val_idx] \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mpredict(X_va, batch_size\u001b[38;5;241m=\u001b[39mBATCH)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/keras/engine/training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1432\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1433\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1434\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1443\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1444\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1445\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1458\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/keras/engine/training.py:1753\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1753\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1755\u001b[0m       callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/keras/engine/data_adapter.py:1248\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1249\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1250\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m   1253\u001b[0m     original_spe)\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# 0.  PREP\n",
    "# ------------------------------------------\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# X  : all engineered features (numeric dataframe)\n",
    "# y  : target\n",
    "X = train.drop(columns=[\"winner_ratio\"])\n",
    "y = train[\"winner_ratio\"].values.astype(\"float32\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1.  STANDARDISE  (fit only on train folds)\n",
    "# ------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2.  MODEL BUILDER\n",
    "# ------------------------------------------\n",
    "def make_model(input_dim, hidden=[256,128,64], drop=0.20):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    for h in hidden:\n",
    "        model.add(layers.Dense(h, activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))       # ‚¨ÖÔ∏è ensures 0‚Äë1\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mse\",                                        # Kaggle metric\n",
    "        metrics=[],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3.  K‚ÄëFOLD TRAINING\n",
    "# ------------------------------------------\n",
    "N_SPLITS = 5\n",
    "BATCH    = 256\n",
    "EPOCHS   = 300\n",
    "\n",
    "kf   = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "oof  = np.zeros(len(X))\n",
    "pred = np.zeros(len(test))          # will accumulate test predictions\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"\\nüìÇ Fold {fold}\")\n",
    "\n",
    "    # 3‚Äëa  fit scaler on *training* rows only\n",
    "    X_tr = scaler.fit_transform(X.iloc[tr_idx])\n",
    "    X_va = scaler.transform(X.iloc[val_idx])\n",
    "\n",
    "    # 3‚Äëb  build / train\n",
    "    net = make_model(X_tr.shape[1])\n",
    "    es  = keras.callbacks.EarlyStopping(\n",
    "            patience=20, restore_best_weights=True, verbose=0)\n",
    "    net.fit(X_tr, y[tr_idx],\n",
    "            validation_data=(X_va, y[val_idx]),\n",
    "            epochs=EPOCHS, batch_size=BATCH,\n",
    "            callbacks=[es], verbose=0)\n",
    "\n",
    "    # 3‚Äëc  out‚Äëof‚Äëfold preds & score\n",
    "    oof[val_idx] = net.predict(X_va, batch_size=BATCH).squeeze()\n",
    "    y_tr_pred = net.predict(X_tr, batch_size=BATCH).squeeze()\n",
    "    y_val_pred = net.predict(X_va, batch_size=BATCH).squeeze()\n",
    "\n",
    "    train_mse = mean_squared_error(y[tr_idx], y_tr_pred)\n",
    "    val_mse = mean_squared_error(y[val_idx], y_val_pred)\n",
    "\n",
    "    oof[val_idx] = y_val_pred\n",
    "\n",
    "    print(f\"   fold {fold} TRAIN MSE = {train_mse:.6f}\")\n",
    "    print(f\"   fold {fold} VAL   MSE = {val_mse:.6f}\")\n",
    "\n",
    "    mse = mean_squared_error(y[val_idx], oof[val_idx])\n",
    "    print(f\"   fold¬†{fold} MSE = {mse:.6f}\")\n",
    "\n",
    "    # 3‚Äëd  test predictions (scale test once per fold)\n",
    "    X_test_scaled = scaler.transform(test[X.columns])\n",
    "    pred += net.predict(X_test_scaled, batch_size=BATCH).squeeze() / N_SPLITS\n",
    "\n",
    "print(\"\\nüìà CV  MSE  =\", mean_squared_error(y, oof))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4.  SUBMISSION\n",
    "# ------------------------------------------\n",
    "sub = pd.DataFrame({\"index\": test.index, \"winner_ratio\": pred.clip(0,1)})\n",
    "sub.to_csv(\"nn_sigmoid_submission.csv\", index=False)\n",
    "print(\"Submission saved: nn_sigmoid_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Try a gradient‚Äëboosting model built for categorical¬†+¬†numerical**<br> CatBoost \n",
    "# # (or LightGBM¬†w/¬†categorical) usually wins on these ‚Äúmessy mix‚Äù tables. \n",
    "# # | Handles remaining categories natively, robust to skew, requires little tuning. \n",
    "\n",
    "# from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# cat_cols = [\"contributor_type\"]           # now the only remaining string col\n",
    "# train_pool = Pool(data=train.drop(\"winner_ratio\", axis=1),\n",
    "#                   label=train[\"winner_ratio\"],\n",
    "#                   cat_features=[train.columns.get_loc(c) for c in cat_cols])\n",
    "# model = CatBoostRegressor(\n",
    "#         depth=6, learning_rate=0.05, iterations=3000,\n",
    "#         loss_function=\"RMSE\", eval_metric=\"RMSE\",\n",
    "#         early_stopping_rounds=200, verbose=200)\n",
    "# model.fit(train_pool, eval_set=train_pool)   # quick CV ‚Üí use KFold for final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
