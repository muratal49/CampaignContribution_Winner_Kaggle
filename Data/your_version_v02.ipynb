{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/yfvk0l4921v0kg94n8qt42wc0000gn/T/ipykernel_7713/4182173648.py:42: DtypeWarning: Columns (31,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_raw = pd.read_csv(\"training_data.csv\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h5/yfvk0l4921v0kg94n8qt42wc0000gn/T/ipykernel_7713/4182173648.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0ms_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_stz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontributor_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"100_s_d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;31m# Federal‑level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0mf_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_stz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontributor_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederal_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederal_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"100_f_d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contributor_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"zip_code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_stz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"contributor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zip_code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10829\u001b[0m     ) -> DataFrame:\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_macos/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kaggle campaign contribution lab – combined script\n",
    "Created from photographed notebook cells.\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0. Imports & settings\n",
    "# ------------------------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Load raw CSV files\n",
    "# ------------------------------------------------------------------\n",
    "# Bipartite state‑level contribution networks\n",
    "all_candidates_state_bi = pd.read_csv(\"all_candidates_state_bipartite_weighted_network.csv\")\n",
    "winning_candidates_state_bi = pd.read_csv(\"winning_candidates_state_bipartite_weighted_network.csv\")\n",
    "\n",
    "for df in (all_candidates_state_bi, winning_candidates_state_bi):\n",
    "    df.index = df[\"Unnamed: 0\"]\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"Unnamed: 1\"], inplace=True)\n",
    "\n",
    "# Top‑100 contributor networks\n",
    "federal_net = pd.read_csv(\"federal_contributor_top100_contributors_network.csv\")\n",
    "state_net   = pd.read_csv(\"state_contributor_top100_contributors_network.csv\")\n",
    "for df in (federal_net, state_net):\n",
    "    df.index = df[\"Unnamed: 0\"]\n",
    "    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Tabular train/test sets\n",
    "train_raw = pd.read_csv(\"training_data.csv\")\n",
    "test_raw  = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Build bipartite graphs & projected state graphs\n",
    "# ------------------------------------------------------------------\n",
    "def build_projected_state_graph(df):\n",
    "    \"\"\"Return projected homogeneous state graph with custom weights.\"\"\"\n",
    "    B = nx.Graph()\n",
    "    states     = list(df.columns)\n",
    "    candidates = list(df.index)\n",
    "\n",
    "    B.add_nodes_from(candidates, bipartite=0)\n",
    "    B.add_nodes_from(states,     bipartite=1)\n",
    "\n",
    "    for s in states:\n",
    "        for c in df[df[s] > 0].index:\n",
    "            B.add_edge(s, c, weight=df.loc[c, s])\n",
    "\n",
    "    def my_weight(G, u, v, weight=\"weight\"):\n",
    "        w = 0\n",
    "        for nbr in set(G[u]) & set(G[v]):\n",
    "            w += G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1)\n",
    "        return w\n",
    "\n",
    "    bottom_nodes = {n for n, d in B.nodes(data=True) if d[\"bipartite\"] == 1}\n",
    "    return bipartite.generic_weighted_projected_graph(B, bottom_nodes,\n",
    "                                                      weight_function=my_weight)\n",
    "\n",
    "G_all = build_projected_state_graph(all_candidates_state_bi)\n",
    "G_win = build_projected_state_graph(winning_candidates_state_bi)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Helper to parse “top‑100” contributor column names\n",
    "# ------------------------------------------------------------------\n",
    "def parse_top100_cols(df_net):\n",
    "    meta = {}\n",
    "    for col in df_net.columns:\n",
    "        name = \" \".join(col.split(\" \")[:-2])\n",
    "        contr_type = \"Non-Individual\" if re.search(\",\", name) is None else \"Individual\"\n",
    "        zip_code = col.split(\" \")[-2] or 0\n",
    "        state = col.split(\" \")[-1]\n",
    "        meta[col] = {\"name\": name, \"zip_code\": zip_code, \"state\": state,\n",
    "                     \"contributor_type\": contr_type}\n",
    "    return meta\n",
    "\n",
    "federal_meta = parse_top100_cols(federal_net)\n",
    "state_meta   = parse_top100_cols(state_net)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Merge raw train/test into a single “all_data” frame\n",
    "# ------------------------------------------------------------------\n",
    "train_raw[\"train_label\"] = 1\n",
    "test_raw[\"train_label\"]  = 0\n",
    "train_raw[\"index\"]       = train_raw.index\n",
    "test_raw[\"index\"]        = test_raw.index\n",
    "\n",
    "all_data = pd.concat([train_raw, test_raw], axis=0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Add graph‑based centrality & community features\n",
    "# ------------------------------------------------------------------\n",
    "def add_degree(df, graph, col, weight=None):\n",
    "    deg = pd.DataFrame(nx.degree(graph, weight=weight), columns=[\"state\", col])\n",
    "    return df.merge(deg, how=\"left\")\n",
    "\n",
    "all_data = (all_data\n",
    "            .pipe(add_degree, G_all, \"d_all\")\n",
    "            .pipe(add_degree, G_win, \"d_win\")\n",
    "            .pipe(add_degree, G_all, \"d_weight_all\", weight=\"weight\")\n",
    "            .pipe(add_degree, G_win, \"d_weight_win\", weight=\"weight\"))\n",
    "\n",
    "# Community clustering with greedy modularity\n",
    "def add_communities(df, graph, prefix):\n",
    "    groups = nx.community.greedy_modularity_communities(graph, weight=\"weight\")\n",
    "    lbl = np.full(len(df), -1, dtype=int)\n",
    "    num = np.full(len(df), -1, dtype=int)\n",
    "    for cid, nodes in enumerate(groups):\n",
    "        m = df[\"state\"].isin(nodes)\n",
    "        lbl[m] = cid\n",
    "        num[m] = len(nodes)\n",
    "    df[f\"{prefix}\"] = lbl\n",
    "    df[f\"{prefix}_num\"] = num\n",
    "    return df\n",
    "\n",
    "all_data = add_communities(all_data, G_all, \"greedy_modularity\")\n",
    "all_data = add_communities(all_data, G_win, \"greedy_modularity_win\")\n",
    "\n",
    "# Louvain clustering\n",
    "def add_louvain(df, graph, prefix):\n",
    "    groups = nx.community.louvain_communities(graph)\n",
    "    lbl = np.full(len(df), -1, dtype=int)\n",
    "    num = np.full(len(df), -1, dtype=int)\n",
    "    for cid, nodes in enumerate(groups):\n",
    "        m = df[\"state\"].isin(nodes)\n",
    "        lbl[m] = cid\n",
    "        num[m] = len(nodes)\n",
    "    df[f\"{prefix}\"] = lbl\n",
    "    df[f\"{prefix}_num\"] = num\n",
    "    return df\n",
    "\n",
    "all_data = add_louvain(all_data, G_all, \"louvain_communities\")\n",
    "all_data = add_louvain(all_data, G_win, \"louvain_communities_win\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Top‑100 contributor degree‑based features\n",
    "# ------------------------------------------------------------------\n",
    "def contributor_features(net, meta, prefix):\n",
    "    d = pd.DataFrame(nx.degree(nx.from_pandas_adjacency(net), weight=\"weight\"),\n",
    "                     columns=[\"name\", \"d\"])\n",
    "\n",
    "    # Build individual lists for state, type, zip_code\n",
    "    state_list, type_list, zip_list = [], [], []\n",
    "    for name in d[\"name\"]:\n",
    "        meta_row = meta.get(name, {\"state\": None, \"contributor_type\": None, \"zip_code\": None})\n",
    "        state_list.append(meta_row[\"state\"])\n",
    "        type_list.append(meta_row[\"contributor_type\"])\n",
    "        zip_list.append(meta_row[\"zip_code\"])\n",
    "\n",
    "    d[f\"state_{prefix}\"] = state_list\n",
    "    d[f\"type_{prefix}\"] = type_list\n",
    "    d[f\"zip_{prefix}\"] = zip_list\n",
    "\n",
    "    # Group means — only on 'd' (degree)\n",
    "    state = (\n",
    "        d.groupby(f\"state_{prefix}\")[\"d\"]\n",
    "        .mean().reset_index().rename(columns={\"d\": f\"state_{prefix}\"})\n",
    "    )\n",
    "    type_ = (\n",
    "        d.groupby(f\"type_{prefix}\")[\"d\"]\n",
    "        .mean().reset_index().rename(columns={\"d\": f\"type_{prefix}\"})\n",
    "    )\n",
    "    zip_ = (\n",
    "        d.groupby(f\"zip_{prefix}\")[\"d\"]\n",
    "        .mean().reset_index().rename(columns={\"d\": f\"zip_{prefix}\"})\n",
    "    )\n",
    "    zip_[f\"zip_{prefix}\"] = zip_[f\"zip_{prefix}\"].astype(float)\n",
    "\n",
    "    # Group by all three categorical keys\n",
    "    state_type_zip = (\n",
    "        d.groupby([f\"state_{prefix}\", f\"type_{prefix}\", f\"zip_{prefix}\"])[\"d\"]\n",
    "        .mean().reset_index().rename(columns={\"d\": f\"mean_money_{prefix}\"})\n",
    "    )\n",
    "    state_type_zip[\"zip_code\"] = state_type_zip[f\"zip_{prefix}\"].astype(float)\n",
    "\n",
    "    return state, type_, zip_, state_type_zip\n",
    "\n",
    "# State‑level\n",
    "s_state, s_type, s_zip, s_stz = contributor_features(state_net, state_meta, \"100_s_d\")\n",
    "# Federal‑level\n",
    "f_state, f_type, f_zip, f_stz = contributor_features(federal_net, federal_meta, \"100_f_d\")\n",
    "\n",
    "all_data = all_data.merge(s_state, how=\"left\", on=\"state\")\n",
    "all_data = all_data.merge(s_type, how=\"left\", on=\"contributor_type\")\n",
    "all_data = all_data.merge(s_zip, how=\"left\", on=\"zip_code\")\n",
    "all_data = all_data.merge(s_stz, how=\"left\", on=[\"state\", \"contributor_type\", \"zip_code\"])\n",
    "\n",
    "all_data = all_data.merge(f_state, how=\"left\", on=\"state\")\n",
    "all_data = all_data.merge(f_type, how=\"left\", on=\"contributor_type\")\n",
    "all_data = all_data.merge(f_zip, how=\"left\", on=\"zip_code\")\n",
    "all_data = all_data.merge(f_stz, how=\"left\", on=[\"state\", \"contributor_type\", \"zip_code\"])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7. Encode categorical columns, clean NA/inf\n",
    "# ------------------------------------------------------------------\n",
    "def ordinalize(df, col):\n",
    "    labels = df[col].astype(str).unique().tolist()\n",
    "    df[col] = df[col].astype(str).apply(lambda x: labels.index(x))\n",
    "    return df\n",
    "\n",
    "for col in (\"general_sector\", \"city\", \"zip_code\",\n",
    "            \"specific_sector\", \"state\", \"contributor_type\"):\n",
    "    if col in all_data.columns:\n",
    "        all_data = ordinalize(all_data, col)\n",
    "\n",
    "all_data = all_data.replace(\"#NAME?\", 0)\n",
    "all_data[np.isinf(all_data)] = 0\n",
    "all_data = all_data.fillna(0)\n",
    "\n",
    "# One‑hot encode selected discrete vars\n",
    "all_data = pd.get_dummies(\n",
    "    all_data,\n",
    "    columns=[\"specific_sector\", \"state\", \"contributor_type\",\n",
    "             \"greedy_modularity\", \"greedy_modularity_win\",\n",
    "             \"louvain_communities\", \"louvain_communities_win\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 8. Train/test/validation split for modeling\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First separate the training data from the final test data\n",
    "train_df = all_data[all_data[\"train_label\"] == 1].drop(columns=[\"index\"])\n",
    "test_df = all_data[all_data[\"train_label\"] == 0]\n",
    "\n",
    "# Now split the training data into train and validation sets (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare X and y for each set\n",
    "y_train = train_df[\"winner_ratio\"]\n",
    "X_train = train_df.drop(columns=[\"winner_ratio\", \"train_label\"])\n",
    "\n",
    "y_val = val_df[\"winner_ratio\"]\n",
    "X_val = val_df.drop(columns=[\"winner_ratio\", \"train_label\"])\n",
    "\n",
    "test_idx = test_df[\"index\"]\n",
    "X_test = test_df.drop(columns=[\"winner_ratio\", \"train_label\", \"index\"])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9. Train Random Forest model directly (no hyperparameter tuning)\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Create and train a Random Forest model with default parameters\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on train and validation sets\n",
    "train_pred = rf.predict(X_train)\n",
    "val_pred = rf.predict(X_val)\n",
    "\n",
    "# Calculate RMSE for both sets\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, train_pred))\n",
    "val_rmse = np.sqrt(metrics.mean_squared_error(y_val, val_pred))\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse:,.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:,.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Plot actual vs predicted values for train and validation data\n",
    "# ------------------------------------------------------------------\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training data plot\n",
    "ax1.scatter(y_train, train_pred, alpha=0.5)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title(f'Training Data: Actual vs Predicted\\nRMSE: {train_rmse:.4f}')\n",
    "\n",
    "# Validation data plot\n",
    "ax2.scatter(y_val, val_pred, alpha=0.5)\n",
    "ax2.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')\n",
    "ax2.set_xlabel('Actual Values')\n",
    "ax2.set_ylabel('Predicted Values')\n",
    "ax2.set_title(f'Validation Data: Actual vs Predicted\\nRMSE: {val_rmse:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('actual_vs_predicted.png')\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10. Predict test & save submission\n",
    "# ------------------------------------------------------------------\n",
    "test_pred = rf.predict(X_test)\n",
    "submission = pd.DataFrame({\"index\": test_idx, \"winner_ratio\": test_pred})\n",
    "submission.to_csv(\"sample_submission_combined_RF.csv\", index=False)\n",
    "print(\"Submission saved → sample_submission_combined_RF.csv\")\n",
    "print(\"Plot saved → actual_vs_predicted.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 8. Train/test split for modeling\n",
    "# ------------------------------------------------------------------\n",
    "train_df = all_data[all_data[\"train_label\"] == 1].drop(columns=[\"index\"])\n",
    "test_df  = all_data[all_data[\"train_label\"] == 0]\n",
    "\n",
    "y_train = train_df[\"winner_ratio\"]\n",
    "X_train = train_df.drop(columns=[\"winner_ratio\", \"train_label\"])\n",
    "test_idx = test_df[\"index\"]\n",
    "X_test  = test_df.drop(columns=[\"winner_ratio\", \"train_label\", \"index\"])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9. Random Forest + RandomizedSearchCV\n",
    "# ------------------------------------------------------------------\n",
    "param_dists = {\n",
    "    \"n_estimators\": range(1, 50, 5),\n",
    "    \"max_depth\": range(1, 30, 5),\n",
    "    \"min_samples_split\": [2, 3, 4],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf_cv = RandomizedSearchCV(\n",
    "    rf, param_dists, n_iter=300, cv=3,\n",
    "    scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1\n",
    ")\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, rf_cv.predict(X_train)))\n",
    "print(f\"Training RMSE: {train_rmse:,.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10. Predict test & save submission\n",
    "# ------------------------------------------------------------------\n",
    "test_pred = rf_cv.predict(X_test)\n",
    "submission = pd.DataFrame({\"index\": test_idx, \"winner_ratio\": test_pred})\n",
    "submission.to_csv(\"sample_submission_combined_RF.csv\", index=False)\n",
    "print(\"Submission saved → sample_submission_combined_RF.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
